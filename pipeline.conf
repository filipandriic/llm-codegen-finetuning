# ---- DATASET ----
ROOTS=/Users/filipandric/Downloads/std-proj/tren
DATASET_DIR=/Users/filipandric/codellama_finetune/dataset

MAKE_DATASET=python make_dataset.py
DATASET_MODES=file diff
SAMPLES_PER_PROJECT=60
MAX_RELEVANT_FILES=4
MAX_FILE_BYTES=6000
FORMAT=inst
VAL_FRAC=0.08

# ---- TRAIN (choose base + out) ----
# -- deepseek-ai/deepseek-coder-1.3b-instruct --
# -- TinyLlama/TinyLlama-1.1B-Chat-v1.0 --
TRAIN=python train_lora.py
TRAIN_DATA=$(DATASET_DIR)/train.jsonl
BASE_MODEL=TinyLlama/TinyLlama-1.1B-Chat-v1.0
LORA_OUT=lora_model
SEQ_LEN=512
EPOCHS=3
LR=2e-4
BATCH_SIZE=1
GRAD_ACCUM=32
LORA_R=8
LORA_ALPHA=16
NO_PACKING=--no-packing
GRAD_CHECKPOINTING=--grad-checkpointing

# ---- FOLDER ----
ifeq ($(findstring deepseek-ai/deepseek-coder-1.3b-instruct,$(BASE_MODEL)),deepseek-ai/deepseek-coder-1.3b-instruct)
  MODEL_TAG := deepseek
else
  MODEL_TAG := tinyllama
endif
EPOCH_TAG := $(EPOCHS)ep
ART_DIR   := artifacts/$(MODEL_TAG)-$(EPOCH_TAG)

# ---- EVAL ----
EVAL_PPL  := python eval_perplexity.py
VAL_DATA  := $(DATASET_DIR)/val.jsonl

# ---- MERGE ----
MERGE := python merge_lora.py
MERGE_BASE=$(BASE_MODEL)
MERGE_ADAPTER=$(LORA_OUT)
MERGED_HF := $(ART_DIR)/merged_hf

# ---- CONVERT (llama.cpp) ----
LLAMACPP_DIR=/Users/filipandric/llama.cpp
GGUF_OUT  := $(ART_DIR)/model_finetuned.gguf

# ---- QUANT ----
QUANT_METHOD=Q5_K_M
QUANT_OUT := $(ART_DIR)/model_finetuned.$(QUANT_METHOD).gguf
